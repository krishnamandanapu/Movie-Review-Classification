{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_file = \"/Users/krishna/workspace/python/CS584/HW1_(584)_Movie_Review_Classification/train.dat\"\n",
    "test_file = \"/Users/krishna/workspace/python/CS584/HW1_(584)_Movie_Review_Classification/test.dat\"\n",
    "\n",
    "train_array = np.genfromtxt(train_file, usecols=(0, 1), dtype=None, encoding=\"UTF-8\", delimiter='\\t')\n",
    "train_data = pd.DataFrame(train_array)\n",
    "train_data.columns = [\"Sentiment\", \"Review\"]\n",
    "\n",
    "\n",
    "test_array = np.genfromtxt(test_file, usecols=(0), encoding=\"UTF-8\", dtype=None, delimiter=\"#EOL\")\n",
    "test_data = pd.DataFrame(test_array)\n",
    "test_data.columns = [\"Review\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>One of my all-time favorite so-laughably-lousy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>I had high hopes for this film, because I thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>When this was released, I thought this was one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>I just watched this movie on Starz. Let me go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I loved it so much that I bought the DVD and t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                             Review\n",
       "0          1  One of my all-time favorite so-laughably-lousy...\n",
       "1         -1  I had high hopes for this film, because I thou...\n",
       "2         -1  When this was released, I thought this was one...\n",
       "3         -1  I just watched this movie on Starz. Let me go ...\n",
       "4          1  I loved it so much that I bought the DVD and t..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a very low budget film, set in one loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One minute into THE UNTOLD and it`s already ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently purchased this on DVD as I hadn't h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some people have the ability to use only 3 neu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As I've said in the title of this review, It p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  This is a very low budget film, set in one loc...\n",
       "1  One minute into THE UNTOLD and it`s already ri...\n",
       "2  I recently purchased this on DVD as I hadn't h...\n",
       "3  Some people have the ability to use only 3 neu...\n",
       "4  As I've said in the title of this review, It p..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now we'll clean & then pre-process the text data. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/krishna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/krishna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def pre_process(review):\n",
    "    # 1. Remove HTML tags\n",
    "    text_only = BeautifulSoup(review, features=\"html.parser\").get_text() \n",
    "    \n",
    "    # 2. Removing Accented Text\n",
    "    new_text = unicodedata.normalize('NFKD', text_only).encode('ascii', 'ignore').decode('UTF-8', 'ignore')\n",
    "    \n",
    "    # 3. Remove Email IDs, URLs and numbers\n",
    "    noEmail = re.sub(r'([\\w\\.-]+@[\\w\\.-]+\\.\\w+)','',new_text)\n",
    "    \n",
    "    noUrl = re.sub(r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]| \\\n",
    "        [a-z0-9.\\-]+[.][a-z]{2,4}/|[a-z0-9.\\-]+[.][a-z])(?:[^\\s()<>]+|\\(([^\\s()<>]+| \\\n",
    "        (\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))','', noEmail)\n",
    "    \n",
    "    # 4. Tokenize\n",
    "    toktok = ToktokTokenizer()\n",
    "    words= toktok.tokenize(noUrl)\n",
    "    \n",
    "    # 5. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "    # 6. Remove stop words and also 3-letter words and Lemmatize the review\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = ''\n",
    "    lemmatized_words = [str(lemmatizer.lemmatize(word)) + ' ' for word in words if word not in stops and len(word) > 3]\n",
    "    \n",
    "    return \"\".join(lemmatized_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Creating a matrix of word counts using TF-IDF Vectorization </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(train_data, test_data):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(norm = 'l2', lowercase = True, min_df = 0, \n",
    "                                       use_idf = True, smooth_idf = False, sublinear_tf = True, \\\n",
    "                                       ngram_range=(1,2))\n",
    "    train_vector = tfidf_vectorizer.fit_transform(train_data)\n",
    "\n",
    "    test_vector = tfidf_vectorizer.transform(test_data)\n",
    "    \n",
    "    return train_vector, test_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Finding Similarities using Cosine Similarities</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity(train_vector, test_vector):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    csr_sim = cosine_similarity(test_vector,train_vector)\n",
    "    return csr_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Finding Nearest Neighbours and Predicting the Sentiment. KNN </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(similarities, labels, k):\n",
    "    \n",
    "    # First let us find the nearest neighbours\n",
    "    k = k\n",
    "    predictions = list()\n",
    "\n",
    "    for similarity in similarities:\n",
    "        nearest_neighbours = np.argsort(-similarity)[:k] # Here we are sorting & taking the indices\n",
    "        \n",
    "        # Now Check the sentiments of these neighbours\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        for neighbor in nearest_neighbours:\n",
    "            if int(labels[neighbor]) == 1:\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "                \n",
    "        # Now we'll consider the majority vote\n",
    "        if positive_count > negative_count:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def iter_row(df, col):\n",
    "#     for i, row in df.iterrows():\n",
    "#         val = row[col]\n",
    "#         df.at[i, col] = pre_process(val)\n",
    "# import time\n",
    "# start_time = time.clock()\n",
    "# iter_row(train_data, \"Review\")\n",
    "# end_time = time.clock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use this when predicting an unseen test data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6 s ± 61.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "19.4 s ± 454 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Use this when predicting an unseen test file\n",
    "\n",
    "#### Preprocessing\n",
    "%timeit train_data[\"Review\"] = train_data[\"Review\"].apply(pre_process)\n",
    "%timeit test_data[\"Review\"] = test_data[\"Review\"].apply(pre_process)\n",
    "\n",
    "train_vector, test_vector = create_vector(train_data[\"Review\"], test_data[\"Review\"])\n",
    "similarities = find_similarity(train_vector, test_vector)\n",
    "predictions = predict(similarities, train_data[\"Sentiment\"], k=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use this to Measure the accuracy for known test data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Fold Cross Validation (Used for measuring)\n",
    "\n",
    "# import copy \n",
    "# accuracy_list = []\n",
    "# k_fold = 5\n",
    "# data_set = copy.deepcopy(train_array)\n",
    "# for _ in range(k_fold):\n",
    "#     fold_size = int((len(data_set) / k_fold))\n",
    "    \n",
    "    \n",
    "#     new_train_array = data_set[fold_size:]\n",
    "#     new_train_data = pd.DataFrame(new_train_array)\n",
    "#     new_train_data.columns = [\"Sentiment\", \"Review\"]\n",
    "    \n",
    "#     new_test_array = data_set[:fold_size]\n",
    "#     new_test_data = pd.DataFrame(new_test_array)\n",
    "#     new_test_data.columns = [\"Sentiment\", \"Review\"]\n",
    "\n",
    "#### Preprocessing\n",
    "# #     new_train_data[\"Review\"] = new_train_data[\"Review\"].apply(pre_process)\n",
    "# #     new_test_data[\"Review\"] = new_test_data[\"Review\"].apply(pre_process)\n",
    "\n",
    "\n",
    "#     train_vector, test_vector = create_vector(new_train_data[\"Review\"], new_test_data[\"Review\"])\n",
    "\n",
    "\n",
    "#     similarities = find_similarity(train_vector, test_vector)\n",
    "\n",
    "#     predictions = predict(similarities, new_train_data[\"Sentiment\"], k=250)\n",
    "\n",
    "#     from sklearn import metrics\n",
    "#     #     print(metrics.confusion_matrix(new_test_data[\"Sentiment\"], predictions))\n",
    "#     accuracy = metrics.accuracy_score(new_test_data[\"Sentiment\"], predictions)\n",
    "#     print(\"Accuracy for %s is %s\" %(_, accuracy))\n",
    "#     accuracy_list.append(accuracy)\n",
    "    \n",
    "#     data_set = np.delete(data_set, slice(0, fold_size), axis=0)\n",
    "#     data_set = np.concatenate([new_train_array, new_test_array])\n",
    "    \n",
    "# print(accuracy_list)\n",
    "# np.mean(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to write to file\n",
    "with open('/Users/krishna/Downloads/Fall_21/CS_584_DM/format.dat', 'w') as log:\n",
    "    for x in predictions:\n",
    "        log.write(str(x)+'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv DM",
   "language": "python",
   "name": "venv_dm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
